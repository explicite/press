Hadoop
MapReduce
11 May 2017
Tags: hadoop, mapreduce, hdfs, java

Jan Paw
Developer, VirtusLab
jpaw@virtuslab.com
http://virtuslab.com

draft

* What Is Apache Hadoop?

- OS implementation MapReduce
.link https://research.google.com/archive/mapreduce.html Google - Appeared on 2004 Dec

 programming model for processing and generating large data sets

- First commit on 2005 May and still developed

- Alternatives
.link http://flink.apache.org/ Flink
.link http://spark.apache.org/ Spark
.link http://storm.apache.org/ Storm
.link https://streamsets.com/ StreamSets

* What Hadoop Is Not
- Apache Hadoop is not a substitute for a database
- MapReduce is not always the best algorithm
- HDFS is not a complete POSIX filesystem
- ...

* What Is MapReduce?

- Programming model 

For processing and generating large data sets.

* MapReduce

.image img/mapreduce.png 170 _

.caption MapReduce pipeline
.caption Mapper (Key, Value) -> (Key, Value)
.caption Reducer (Key, Iterable[Value]) -> (Key, Value)
.caption Partitioning | Sorting | Grouping (Key, Value) -> (Key, Iterable[Value])

* HDFS 

.image img/hdfs.png 400 _

.link https://www.youtube.com/watch?v=4Gfl0WuONMY YouTube - Understanding HDFS using Legos
.link https://research.google.com/archive/gfs.html Google File System

* HDFS - write

.image img/write.png 400 _

* HDFS - read

.image img/read.png 400 _

* Hadoop modules

- Hadoop Common
- Hadoop Distributed File System (HDFS)
- Hadoop YARN
- Hadoop MapReduce
- ...

* Hadoop API
- Java (JVM)
- Python
- Ruby
- ...
- Any executable on cluster nodes ;)
- Hadoop job

* Hadoop API
 hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.1.2.jar \
        -input /ngrams \
        -output /output-streaming \
        -mapper mapper.py \
        -combiner reducer.py \
        -reducer reducer.py \
        -jobconf stream.num.map.output.key.fields=3 \
        -jobconf stream.num.reduce.output.key.fields=3 \
        -jobconf mapred.reduce.tasks=10 \
        -file mapper.py \
        -file reducer.py

.link https://github.com/cloudera/python-ngrams full source code

* Workshop

- Run hadoop in docker
 you@localhost docker-compose up

- Check docker hash
 you@localhost docker ps

- Exec bash on hadoop host
 you@localhost docker exec -it `hash` bash

- Move to hadoop dir
 hadoop@docker cd $HADOOP_PREFIX

* Testing

- Run the mapreduce example
 hadoop@docker bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar \
 grep input output 'dfs[a-z.]+'

- Check output
 hadoop@docker bin/hdfs dfs -cat output/*

* MapReduce

- Open file
 com.virtuslab.MapReduceExample.java

- Reducer

- Mapper

* MapReduce

- Copy input to docker
 you@localhost cp src/main/resources/mapreduce ~/input

- Add file to hdfs
 hadoop@docker bin/hadoop fs -mkdir input //if not exist
 hadoop@docker bin/hadoop fs -put input/mapreduce input

- Create ouput dir
 hadoop@docker bin/hadoop fs -mkdir output //if not exist

* MapReduce

- Compile source
 you@localhost sbt assembly

- Copy *.jar to docker
 you@localhost cp target/hadoop-example-2.7.1.jar ~/jobs

- Run job
 hadoop@docker bin/hadoop jar jobs/hadoop-example-2.7.1.jar input output

 ...
 CONNECTION=0
 IO_ERROR=0
 WRONG_LENGTH=0
 WRONG_MAP=0
 WRONG_REDUCE=0
 File Input Format Counters
 Bytes Read=336
 File Output Format Counters
 Bytes Written=24

* MapReduce
 - Check result
 bin/hadoop fs -copyToLocal output/part-00000 ./otuput/result

 1981	34
 1984	40
 1985	45

* Partitioner
- Open file
 com.virtuslab.PartitionerExample.java

- Partitioner
